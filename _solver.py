class AdamOptimizer:
    def __init__(self, params, learning_rate_init=0.001, beta_1=0.9,
                 beta_2=0.999, epsilon=1e-8):
        pass
        #TODO
    
    def update_params(self, grads):
        pass

    def iteration_ends(self, time_step):
        pass

    def trigger_stopping(self, msg):
        pass
